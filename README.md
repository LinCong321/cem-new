# CEM-new
 
## 0 Background
**Retrieval-Augmented Generation (RAG)** refers to optimizing the output of **large language models (LLM)** so that they can reference authoritative knowledge bases outside the training data source before generating responses.

An important prerequisite for RAG is that we need to store the knowledge base, that is, documents, etc., in a vector database, and this step is completed through **Embedding**.

Considering the massive size of documents, the compression and decompression time required to build and migrate the vector database is difficult to ignore. Therefore, we consider completing the embedding directly in the **compressed state**, which can save time and space complexity.

## 1 Design

## 2 Implementation

## 3 Result

## 4 Future Work
